"""
æ–‡æ¡£å­¦ä¹ å·¥å…· - PDF/æ–‡æœ¬è§£æ
ç”¨äºè¯»å–å¹¶å­¦ä¹ å„ç±»æ–‡æ¡£èµ„æ–™
"""
import os
import re
from pathlib import Path
from typing import List, Dict, Optional

class DocumentLearner:
    """æ–‡æ¡£å­¦ä¹ å™¨ - æ”¯æŒå¤šç§æ ¼å¼"""
    
    def __init__(self, knowledge_base_dir: str = None):
        self.knowledge_base_dir = Path(knowledge_base_dir) if knowledge_base_dir else \
            Path(__file__).parent.parent / "knowledge"
        self.knowledge_base_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_types = {
            ".txt": self._parse_txt,
            ".md": self._parse_txt,
            ".py": self._parse_code,
            ".json": self._parse_txt,
            ".csv": self._parse_txt,
            ".pdf": self._parse_pdf,  # PDFæ”¯æŒ
        }
    
    def learn_file(self, file_path: str) -> Dict:
        """
        å­¦ä¹ å•ä¸ªæ–‡ä»¶
        è¿”å›: æ–‡ä»¶ä¿¡æ¯å’Œå†…å®¹æ‘˜è¦
        """
        path = Path(file_path)
        
        if not path.exists():
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        ext = path.suffix.lower()
        
        if ext not in self.supported_types:
            return {
                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}",
                "supported": list(self.supported_types.keys())
            }
        
        # è°ƒç”¨å¯¹åº”è§£æå™¨
        parser = self.supported_types[ext]
        content = parser(path)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_summary(content)
        
        # è®¡ç®—é¡µæ•°/è¡Œæ•°
        if ext == ".pdf":
            # PDFç”¨é¡µæ•°
            page_count = self._count_pdf_pages(path)
            line_count = len(content.splitlines())
        else:
            page_count = None
            line_count = len(content.splitlines())
        
        # ä¿å­˜åˆ°çŸ¥è¯†åº“
        knowledge_entry = {
            "filename": path.name,
            "path": str(path.absolute()),
            "type": ext,
            "size": path.stat().st_size,
            "content_preview": content[:1000] if len(content) > 1000 else content,
            "summary": summary,
            "line_count": line_count,
            "learned_at": self._get_timestamp()
        }
        
        if page_count:
            knowledge_entry["page_count"] = page_count
        
        self._save_to_knowledge_base(knowledge_entry)
        
        return knowledge_entry
    
    def _count_pdf_pages(self, path: Path) -> int:
        """ç»Ÿè®¡PDFé¡µæ•°"""
        try:
            import pdfplumber
            with pdfplumber.open(path) as pdf:
                return len(pdf.pages)
        except:
            return 0
    
    def learn_directory(self, dir_path: str, recursive: bool = True) -> List[Dict]:
        """
        æ‰¹é‡å­¦ä¹ ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        """
        path = Path(dir_path)
        results = []
        
        if not path.exists():
            return [{"error": f"ç›®å½•ä¸å­˜åœ¨: {dir_path}"}]
        
        pattern = "**/*" if recursive else "*"
        
        for file_path in path.glob(pattern):
            if file_path.is_file() and file_path.suffix.lower() in self.supported_types:
                print(f"æ­£åœ¨å­¦ä¹ : {file_path.name}")
                result = self.learn_file(str(file_path))
                results.append(result)
        
        return results
    
    def _parse_txt(self, path: Path) -> str:
        """è§£ææ–‡æœ¬æ–‡ä»¶"""
        encodings = ["utf-8", "gbk", "gb2312", "utf-16"]
        
        for encoding in encodings:
            try:
                return path.read_text(encoding=encoding)
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨äºŒè¿›åˆ¶è¯»å–
        return path.read_bytes().decode("utf-8", errors="ignore")
    
    def _parse_code(self, path: Path) -> str:
        """è§£æä»£ç æ–‡ä»¶"""
        content = self._parse_txt(path)
        # æå–æ³¨é‡Šå’Œå‡½æ•°å®šä¹‰ä½œä¸ºå­¦ä¹ é‡ç‚¹
        return content
    
    def _parse_pdf(self, path: Path) -> str:
        """è§£æPDFæ–‡ä»¶ - ä½¿ç”¨pdfplumberæå–æ–‡æœ¬"""
        try:
            import pdfplumber
            
            text_parts = []
            with pdfplumber.open(path) as pdf:
                # æå–æ–‡æ¡£ä¿¡æ¯
                meta = pdf.metadata or {}
                
                # éå†æ‰€æœ‰é¡µé¢æå–æ–‡æœ¬
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(f"\n--- ç¬¬{i+1}é¡µ ---\n")
                        text_parts.append(page_text)
                    
                    # é™åˆ¶åªå¤„ç†å‰20é¡µï¼Œé¿å…è¶…å¤§æ–‡æ¡£
                    if i >= 19:
                        text_parts.append("\n... (æ–‡æ¡£è¶…è¿‡20é¡µï¼Œå·²æˆªæ–­)")
                        break
            
            content = "\n".join(text_parts)
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆPDF
            if not content.strip():
                return "[PDFä¸ºæ‰«æç‰ˆæˆ–å›¾ç‰‡ï¼Œæ— æ³•æå–æ–‡æœ¬å†…å®¹]"
            
            return content
            
        except Exception as e:
            return f"[PDFè§£æé”™è¯¯: {str(e)}]"
    
    def _generate_summary(self, content: str, max_length: int = 500) -> str:
        """ç”Ÿæˆå†…å®¹æ‘˜è¦"""
        # ç®€å•æ‘˜è¦ï¼šå–å‰Nä¸ªå­—ç¬¦ï¼Œä¿ç•™å®Œæ•´å¥å­
        if len(content) <= max_length:
            return content
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        truncated = content[:max_length]
        last_sentence = max(
            truncated.rfind("ã€‚"),
            truncated.rfind("."),
            truncated.rfind("\n")
        )
        
        if last_sentence > max_length * 0.5:
            return truncated[:last_sentence + 1] + "..."
        
        return truncated + "..."
    
    def _save_to_knowledge_base(self, entry: Dict):
        """ä¿å­˜åˆ°çŸ¥è¯†åº“ç´¢å¼•"""
        import json
        
        index_file = self.knowledge_base_dir / "index.json"
        
        # è¯»å–ç°æœ‰ç´¢å¼•
        index = []
        if index_file.exists():
            try:
                with open(index_file, "r", encoding="utf-8") as f:
                    index = json.load(f)
            except:
                index = []
        
        # æ›´æ–°æˆ–æ·»åŠ æ¡ç›®
        existing = False
        for i, item in enumerate(index):
            if item.get("path") == entry["path"]:
                index[i] = entry
                existing = True
                break
        
        if not existing:
            index.append(entry)
        
        # ä¿å­˜ç´¢å¼•
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def search_knowledge(self, keyword: str) -> List[Dict]:
        """æœç´¢å·²å­¦ä¹ çš„çŸ¥è¯†"""
        import json
        
        index_file = self.knowledge_base_dir / "index.json"
        
        if not index_file.exists():
            return []
        
        with open(index_file, "r", encoding="utf-8") as f:
            index = json.load(f)
        
        results = []
        keyword_lower = keyword.lower()
        
        for entry in index:
            # åœ¨æ–‡ä»¶åå’Œå†…å®¹ä¸­æœç´¢
            if (keyword_lower in entry.get("filename", "").lower() or
                keyword_lower in entry.get("summary", "").lower() or
                keyword_lower in entry.get("content_preview", "").lower()):
                results.append(entry)
        
        return results
    
    def list_learned(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²å­¦ä¹ çš„æ–‡æ¡£"""
        import json
        
        index_file = self.knowledge_base_dir / "index.json"
        
        if not index_file.exists():
            return []
        
        with open(index_file, "r", encoding="utf-8") as f:
            return json.load(f)


def main():
    """æµ‹è¯•æ–‡æ¡£å­¦ä¹ åŠŸèƒ½"""
    print("=" * 50)
    print("æ–‡æ¡£å­¦ä¹ å·¥å…·æµ‹è¯•")
    print("=" * 50)
    
    learner = DocumentLearner()
    
    # æµ‹è¯•ï¼šå­¦ä¹ äº¤æ˜“ç›®å½•ä¸‹çš„ç°æœ‰æ–‡ä»¶
    trading_dir = Path(__file__).parent.parent
    
    print(f"\næ‰«æç›®å½•: {trading_dir}")
    results = learner.learn_directory(str(trading_dir), recursive=False)
    
    print(f"\næˆåŠŸå­¦ä¹  {len(results)} ä¸ªæ–‡ä»¶:")
    for r in results:
        if "error" not in r:
            print(f"  âœ… {r['filename']} ({r['size']} bytes, {r['line_count']} è¡Œ)")
        else:
            print(f"  âŒ {r.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡
    learned = learner.list_learned()
    print(f"\nçŸ¥è¯†åº“ç»Ÿè®¡: å…± {len(learned)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•æœç´¢
    print("\næœç´¢å…³é”®è¯ 'stock':")
    search_results = learner.search_knowledge("stock")
    for r in search_results[:3]:
        print(f"  ğŸ“„ {r['filename']}")


if __name__ == "__main__":
    main()
